{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "24b19fff-8f42-4e9f-a73e-00cff106805a",
      "metadata": {
        "id": "24b19fff-8f42-4e9f-a73e-00cff106805a"
      },
      "source": [
        "# M-Shots Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34723a72-1601-4685-a0ba-bff544425d48",
      "metadata": {
        "id": "34723a72-1601-4685-a0ba-bff544425d48"
      },
      "source": [
        "In this notebook, we'll explore small prompt engineering techniques and recommendations that will help us elicit responses from the models that are better suited to our needs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fba193cc-d8a0-4ad2-8177-380204426859",
      "metadata": {
        "id": "fba193cc-d8a0-4ad2-8177-380204426859"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "import os\n",
        "\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_ = load_dotenv(find_dotenv()) # read local .env file\n",
        "\n",
        "OPENAI_API_KEY  = os.getenv('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "502cfc93-21e0-498f-9650-37bc6ddd514d",
      "metadata": {
        "id": "502cfc93-21e0-498f-9650-37bc6ddd514d"
      },
      "source": [
        "# Formatting the answer with Few Shot Samples.\n",
        "\n",
        "To obtain the model's response in a specific format, we have various options, but one of the most convenient is to use Few-Shot Samples. This involves presenting the model with pairs of user queries and example responses.\n",
        "\n",
        "Large models like GPT-3.5 respond well to the examples provided, adapting their response to the specified format.\n",
        "\n",
        "Depending on the number of examples given, this technique can be referred to as:\n",
        "* Zero-Shot.\n",
        "* One-Shot.\n",
        "* Few-Shots.\n",
        "\n",
        "With One Shot should be enough, and it is recommended to use a maximum of six shots. It's important to remember that this information is passed in each query and occupies space in the input prompt.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a8344712-06d7-4c24-83d8-f36d62926e5e",
      "metadata": {
        "id": "a8344712-06d7-4c24-83d8-f36d62926e5e"
      },
      "outputs": [],
      "source": [
        "# Function to call the model.\n",
        "def return_OAIResponse(user_message, context):\n",
        "    client = OpenAI(\n",
        "    # This is the default and can be omitted\n",
        "    api_key=OPENAI_API_KEY,\n",
        ")\n",
        "\n",
        "    newcontext = context.copy()\n",
        "    newcontext.append({'role':'user', 'content':\"question: \" + user_message})\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=newcontext,\n",
        "            temperature=1,\n",
        "        )\n",
        "\n",
        "    return (response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f611d73d-9330-466d-b705-543667e1b561",
      "metadata": {
        "id": "f611d73d-9330-466d-b705-543667e1b561"
      },
      "source": [
        "In this zero-shots prompt we obtain a correct response, but without formatting, as the model incorporates the information he wants."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "647790be-fdb8-4692-a82e-7e3a0220f72a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "647790be-fdb8-4692-a82e-7e3a0220f72a",
        "outputId": "55f14c71-c963-46be-efa4-c1babed10c43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sebastian Vettel won the Formula 1 World Championship in 2010 driving for the Red Bull Racing team.\n"
          ]
        }
      ],
      "source": [
        "#zero-shot\n",
        "context_user = [\n",
        "    {'role':'system', 'content':'You are an expert in F1.'}\n",
        "]\n",
        "print(return_OAIResponse(\"Who won the F1 2010?\", context_user))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e87a9a0a-c1b9-4759-b52f-f6547d29b4c8",
      "metadata": {
        "id": "e87a9a0a-c1b9-4759-b52f-f6547d29b4c8"
      },
      "source": [
        "For a model as large and good as GPT 3.5, a single shot is enough to learn the output format we expect.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "33ac7693-6cf3-44f7-b2ff-55d8a36fe775",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33ac7693-6cf3-44f7-b2ff-55d8a36fe775",
        "outputId": "e5dc4b58-1a86-489d-e82a-1ed28c3488b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Driver: Sebastian Vettel.\n",
            "Team: Red Bull Racing.\n"
          ]
        }
      ],
      "source": [
        "#one-shot\n",
        "context_user = [\n",
        "    {'role':'system', 'content':\n",
        "     \"\"\"You are an expert in F1.\n",
        "\n",
        "     Who won the 2000 f1 championship?\n",
        "     Driver: Michael Schumacher.\n",
        "     Team: Ferrari.\"\"\"}\n",
        "]\n",
        "print(return_OAIResponse(\"Who won the F1 2011?\", context_user))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32c454a8-181b-482b-873a-81d6ffde4674",
      "metadata": {
        "id": "32c454a8-181b-482b-873a-81d6ffde4674"
      },
      "source": [
        "Smaller models, or more complicated formats, may require more than one shot. Here a sample with two shots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "8ce600f7-f92e-4cf7-be4a-408f12eb39d6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ce600f7-f92e-4cf7-be4a-408f12eb39d6",
        "outputId": "10262f1b-fb75-497c-fb1e-d2e58b8144c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Driver: Fernando Alonso.\n",
            "Team: Renault.\n"
          ]
        }
      ],
      "source": [
        "#Few shots\n",
        "context_user = [\n",
        "    {'role':'system', 'content':\n",
        "     \"\"\"You are an expert in F1.\n",
        "\n",
        "     Who won the 2010 f1 championship?\n",
        "     Driver: Sebastian Vettel.\n",
        "     Team: Red Bull Renault.\n",
        "\n",
        "     Who won the 2009 f1 championship?\n",
        "     Driver: Jenson Button.\n",
        "     Team: BrawnGP.\"\"\"}\n",
        "]\n",
        "print(return_OAIResponse(\"Who won the F1 2006?\", context_user))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "4b29898a-f715-46d4-b74b-9f95d3112d38",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4b29898a-f715-46d4-b74b-9f95d3112d38",
        "outputId": "1db13530-a5b7-41a3-d3aa-80561532c6e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The 2019 F1 championship was won by Lewis Hamilton from the Mercedes team.\n"
          ]
        }
      ],
      "source": [
        "print(return_OAIResponse(\"Who won the F1 2019?\", context_user))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZPIZMoLOq6pa",
      "metadata": {
        "id": "ZPIZMoLOq6pa"
      },
      "source": [
        "### üîπ Observations\n",
        "\n",
        "The 2nd time it had right info but wrong format.\n",
        "\n",
        "üß† Why did this happen?\n",
        "\n",
        "Because:\n",
        "\n",
        "- The model is probabilistic, not rule-based ‚Äî it tries to follow patterns but may ‚Äúdrift‚Äù if it‚Äôs confident it knows a better way to express the answer.\n",
        "\n",
        "- Your prompt didn‚Äôt explicitly tell it ‚Äú**always answer** in the same format.‚Äù It only showed examples.\n",
        "\n",
        "- The **`temperature=1`** parameter adds randomness ‚Üí more creativity, less consistency.\n",
        "\n",
        "By Sofia\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f1b71c4-6583-4dcb-b987-02abf6aa4a86",
      "metadata": {
        "id": "5f1b71c4-6583-4dcb-b987-02abf6aa4a86"
      },
      "source": [
        "We've been creating the prompt without using OpenAI's roles, and as we've seen, it worked correctly.\n",
        "\n",
        "However, the proper way to do this is by using these roles to construct the prompt, making the model's learning process even more effective.\n",
        "\n",
        "By not feeding it the entire prompt as if they were system commands, we enable the model to learn from a conversation, which is more realistic for it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "20fa4a25-01a6-4f22-98db-ab7ccc9ba115",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20fa4a25-01a6-4f22-98db-ab7ccc9ba115",
        "outputId": "ed6dee7d-7f7a-4d37-a124-2f7b52d1edff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Driver: Lewis Hamilton. \n",
            "Team: Mercedes. \n",
            "Points: 413.\n"
          ]
        }
      ],
      "source": [
        "#Recomended solution\n",
        "context_user = [\n",
        "    {'role':'system', 'content':'You are and expert in f1.\\n\\n'},\n",
        "    {'role':'user', 'content':'Who won the 2010 f1 championship?'},\n",
        "    {'role':'assistant', 'content':\"\"\"Driver: Sebastian Vettel. \\nTeam: Red Bull. \\nPoints: 256. \"\"\"},\n",
        "    {'role':'user', 'content':'Who won the 2009 f1 championship?'},\n",
        "    {'role':'assistant', 'content':\"\"\"Driver: Jenson Button. \\nTeam: BrawnGP. \\nPoints: 95. \"\"\"},\n",
        "]\n",
        "\n",
        "print(return_OAIResponse(\"Who won the F1 2019?\", context_user))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac6f6b42-f351-496b-a7e8-1286426457eb",
      "metadata": {
        "id": "ac6f6b42-f351-496b-a7e8-1286426457eb"
      },
      "source": [
        "We could also address it by using a more conventional prompt, describing what we want and how we want the format.\n",
        "\n",
        "However, it's essential to understand that in this case, the model is following instructions, whereas in the case of use shots, it is learning in real-time during inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "36c32a32-c348-45b2-85ee-ab4500438c49",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36c32a32-c348-45b2-85ee-ab4500438c49",
        "outputId": "99f1405b-6fb8-4798-de8d-34317c769533"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Driver: Lewis Hamilton\n",
            "Team: Mercedes\n",
            "Points: 413\n"
          ]
        }
      ],
      "source": [
        "context_user = [\n",
        "    {'role':'system', 'content':\"\"\"You are and expert in f1.\n",
        "    You are going to answer the question of the user giving the name of the rider,\n",
        "    the name of the team and the points of the champion, following the format:\n",
        "    Drive:\n",
        "    Team:\n",
        "    Points: \"\"\"\n",
        "    }\n",
        "]\n",
        "\n",
        "print(return_OAIResponse(\"Who won the F1 2019?\", context_user))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "KNDL1GzVngyL",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNDL1GzVngyL",
        "outputId": "809525c6-815b-4c24-b491-12949bbe5a6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Driver: Fernando Alonso.\n",
            "Team: Renault.\n"
          ]
        }
      ],
      "source": [
        "context_user = [\n",
        "    {'role':'system', 'content':\n",
        "     \"\"\"You are classifying .\n",
        "\n",
        "     Who won the 2010 f1 championship?\n",
        "     Driver: Sebastian Vettel.\n",
        "     Team: Red Bull Renault.\n",
        "\n",
        "     Who won the 2009 f1 championship?\n",
        "     Driver: Jenson Button.\n",
        "     Team: BrawnGP.\"\"\"}\n",
        "]\n",
        "print(return_OAIResponse(\"Who won the F1 2006?\", context_user))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qZPNTLMPnkQ4",
      "metadata": {
        "id": "qZPNTLMPnkQ4"
      },
      "source": [
        "Few Shots for classification.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "ejcstgTxnnX5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejcstgTxnnX5",
        "outputId": "c07d608c-be0c-444e-fc09-c6d1102d30e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentiment: Neutral\n"
          ]
        }
      ],
      "source": [
        "context_user = [\n",
        "    {'role':'system', 'content':\n",
        "     \"\"\"You are an expert in reviewing product opinions and classifying them as positive or negative.\n",
        "\n",
        "     It fulfilled its function perfectly, I think the price is fair, I would buy it again.\n",
        "     Sentiment: Positive\n",
        "\n",
        "     It didn't work bad, but I wouldn't buy it again, maybe it's a bit expensive for what it does.\n",
        "     Sentiment: Negative.\n",
        "\n",
        "     I wouldn't know what to say, my son uses it, but he doesn't love it.\n",
        "     Sentiment: Neutral\n",
        "     \"\"\"}\n",
        "]\n",
        "print(return_OAIResponse(\"I'm not going to return it, but I don't plan to buy it again.\", context_user))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ffe1d50b-d262-4e74-8f2d-3559f3fcfb15",
      "metadata": {
        "id": "ffe1d50b-d262-4e74-8f2d-3559f3fcfb15"
      },
      "source": [
        "# Exercise\n",
        " - Complete the prompts similar to what we did in class.\n",
        "     - Try at least 3 versions\n",
        "     - Be creative\n",
        " - Write a one page report summarizing your findings.\n",
        "     - Were there variations that didn't work well? i.e., where GPT either hallucinated or wrong\n",
        " - What did you learn?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "9adda59c-ad09-4e9d-88cd-54f42384a5f3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9adda59c-ad09-4e9d-88cd-54f42384a5f3",
        "outputId": "63f29bb4-97a5-4376-df5a-17cf3f2aa032"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1. Tiergarten: Located in the Mitte district, Tiergarten is one of Berlin's most famous and beloved parks. It is a large green space with walking paths, gardens, and the iconic Victory Column.\n",
            "\n",
            "2. Tempelhofer Feld: Tempelhofer Feld is a former airport turned public park located in the Tempelhof-Sch√∂neberg district. It offers vast open spaces for activities such as cycling, jogging, kite flying, and picnicking.\n",
            "\n",
            "3. Volkspark Friedrichshain: Situated in the Friedrichshain district, this park is the oldest public park in Berlin and features a charming rose garden, a fairytale fountain, and a hill with great views of the city.\n"
          ]
        }
      ],
      "source": [
        "#Few shots\n",
        "context_user = [\n",
        "    {'role':'system', 'content':\n",
        "     \"\"\"You like Berlin and you are a historian.\n",
        "\n",
        "     What are the top 3 clubs?\n",
        "     Club: RSO.\n",
        "     Area: Schoneweide.\n",
        "\n",
        "     Club: Humbolthain\n",
        "     Area: Gesundbrunnen.\n",
        "\n",
        "     Club: Viktoria park\n",
        "     Area: Kreuzberg.\n",
        "\n",
        "     What are the top 3 restaurants?\n",
        "     restaurant: Umami.\n",
        "     Area: Mehringdam\n",
        "\n",
        "     restaurant: NC Kebap.\n",
        "     Area: Baumschulenweg\n",
        "     \"\"\"}\n",
        "]\n",
        "print(return_OAIResponse(\"What are the top 3 famous parks\", context_user))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "njjiphj3u3UE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njjiphj3u3UE",
        "outputId": "fa080c9a-dd11-4e1e-9df7-ca6f0fc8add7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Park: Tiergarten \n",
            "Location: Mitte\n",
            "Park: Tempelhofer Park\n",
            "Location: Tempelhof-Sch√∂neberg\n",
            "Park: Volkspark Friedrichshain \n",
            "Location: Friedrichshain-Kreuzberg\n"
          ]
        }
      ],
      "source": [
        "#Recomended solution\n",
        "context_user = [\n",
        "    {'role':'system', 'content':'You like Berlin and you are a historian.\\n\\n'},\n",
        "    {'role':'user', 'content':'What are the top 3 clubs?'},\n",
        "    {'role':'assistant', 'content':\"\"\"Club: RSO. \\nArea: Schoneweide. \\nClub: Humbolthain \\nArea: Gesundbrunnen. \\nClub: Viktoria park \\nArea: Kreuzberg.\"\"\"},\n",
        "    {'role':'user', 'content':'What are the top 3 restaurants?'},\n",
        "    {'role':'assistant', 'content':\"\"\"restaurant: Umami \\nArea: Mehringdam \\nrestaurant: NC Kebap. \\n.\n",
        "      \"\"\"},\n",
        "]\n",
        "\n",
        "print(return_OAIResponse(\"What are the top 3 famous parks?\", context_user))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "F7y2PQ0kwnSZ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7y2PQ0kwnSZ",
        "outputId": "f65907ea-9da7-4cf1-95b0-aca2472922a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Theater: Deutsches Theater \n",
            "Area: Mitte \n",
            "Theater: Berliner Ensemble \n",
            "Area: Mitte \n",
            "Theater: Volksbuhne \n",
            "Area: Mitte\n"
          ]
        }
      ],
      "source": [
        "context_user = [\n",
        "    {'role':'system', 'content':'You like Berlin and you are a historian.\\n\\n'},\n",
        "\n",
        "    {'role':'user', 'content':'What are the top 3 clubs?'},\n",
        "    {'role':'assistant', 'content':\"\"\"Club: RSO. \\nArea: Schoneweide. \\nClub: Humbolthain \\nArea: Gesundbrunnen. \\nClub: Viktoria park \\nArea: Kreuzberg.\"\"\"},\n",
        "\n",
        "    {'role':'user', 'content':'What are the top 3 restaurants?'},\n",
        "    {'role':'assistant', 'content':\"\"\"Restaurant: Umami \\nArea: Mehringdam \\nRestaurant: NC Kebap \\nArea: Baumschulenweg.\"\"\"},\n",
        "\n",
        "    {'role':'user', 'content':'What are the top 3 museums?'},\n",
        "    {'role':'assistant', 'content':\"\"\"Museum: Pergamonmuseum \\nArea: Mitte \\nMuseum: Neues Museum \\nArea: Mitte \\nMuseum: Deutsches Historisches Museum \\nArea: Mitte.\"\"\"},\n",
        "]\n",
        "\n",
        "print(return_OAIResponse(\"What are the top 3 famous theaters?\", context_user))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DO4BQfa0xEYL",
      "metadata": {
        "id": "DO4BQfa0xEYL"
      },
      "source": [
        "# Report: Few-Shot Prompting for Berlin Locations\n",
        "\n",
        "## Objective\n",
        "The goal of this exercise was to explore how few-shot prompting affects GPT‚Äôs ability to provide structured information about Berlin, including clubs, restaurants, parks, and theaters. I wanted to see how example formatting influences consistency and accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "## Method\n",
        "1. **Few-shot / instruction-only without following GPT model:**  \n",
        "   Initially, I gave GPT a system message describing that it should act as a historian familiar with Berlin. I asked questions about top clubs, restaurants, or parks without providing examples.  \n",
        "   - **Result:** The model seemed to give correct answers, but the formatting varied, sometimes using \"is\" instead of a colon (`:`) to separate fields.\n",
        "\n",
        "2. **Few-shot prompting following the GPT model:**  \n",
        "   I then provided one or two examples for clubs and restaurants in the following format:  \n",
        "   Item: Name\n",
        "   Area: Location\n",
        "\n",
        "- **Observation:** Model responses started matching the provided structure.\n",
        "\n",
        "3. **Adding a third example (museums):**  \n",
        "I extended the context to include museums as a third example, maintaining the same `Item` / `Area` pattern. Then I asked for top theaters.\n",
        "\n",
        "---\n",
        "\n",
        "## Findings / Observations\n",
        "- **Formatting consistency improves with examples:**  \n",
        "- Few-shot answers varied in punctuation and layout.  \n",
        "- Few-shot answers following the GPT model (system, assistant, user)followed the `Item: ... \\nArea: ...` pattern closely.\n",
        "- **Model sometimes repeats areas:**  \n",
        "- Example: All theaters returned ‚ÄúArea: Mitte‚Äù even if the actual location differs slightly ‚Äî minor hallucination likely due to limited examples.\n",
        "- **Correct entities generally returned:**  \n",
        "- The model correctly listed known parks, and theaters, showing strong recall when examples were given.  \n",
        "- **More examples reduce errors:**  \n",
        "- Adding a third example (museums) helped the model generalize formatting to new categories, though content accuracy still depends on its knowledge cut-off.\n",
        "\n",
        "---\n",
        "\n",
        "## Key Learnings\n",
        "1. **Few-shot examples are powerful:** Providing structured examples guides GPT to produce consistent and predictable output.  \n",
        "2. **Explicit formatting matters:** Repeating the desired output format in multiple examples reduces ‚Äúcreative‚Äù deviations.  \n",
        "3. **Hallucination is still possible:** GPT may repeat areas or invent minor details; careful review is needed for critical tasks.  \n",
        "4. **Roles enhance learning:** Using `system`, `user`, and `assistant` roles aligns with the model‚Äôs training and improves pattern adoption.  \n",
        "5. **Zero-shot vs few-shot:** Zero-shot can produce correct answers, but few-shot ensures structural consistency and reliability in formatting.\n",
        "\n",
        "---\n",
        "\n",
        "## Conclusion\n",
        "Few-shot prompting is an effective method for eliciting structured and consistent responses from GPT. By providing multiple examples, the model learns the desired format in real-time during inference. However, while formatting consistency improves, minor factual hallucinations may still occur, so outputs should be verified for accuracy in real-world applications.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
